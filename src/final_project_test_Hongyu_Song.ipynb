{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24677,"status":"ok","timestamp":1614463969000,"user":{"displayName":"Song Hongyu","photoUrl":"","userId":"04662424222752472550"},"user_tz":-480},"id":"QM6_lGRiAqwP","outputId":"cc91b8af-653c-4dbf-d7e5-ec5aa168afcf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Final_project/dataset_amazon/Musical_instruments_reviews.csv\n","/content/drive/MyDrive/Final_project/dataset_amazon/Musical_Instruments_5.json\n","/content/drive/MyDrive/Final_project/dataset_amazon/fuzzydataset.csv\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')#allocation my drive space to this note book, so i can use the path\n","#packages that are useful\n","import numpy as np #linear algebra\n","import pandas as pd #data processing, CSV file I/O (e.g. pd.read_csv)\n","import os #functions for interacting with the operating system\n","import pandas as pd #data analysis toolkit\n","import numpy as np #for array processing\n","import matplotlib.pyplot as plt #plot the figures\n","%matplotlib inline \n","import seaborn as sns#data visualization\n","import re #regularization\n","import nltk #dealing with nature language\n","from nltk.corpus import  stopwords #massive dump of all kinds of natural language data sets\n","from nltk.stem.porter import PorterStemmer #Stem extraction based on porter\n","from nltk.tokenize import word_tokenize, sent_tokenize #using for text spliting\n","from nltk.stem.wordnet import WordNetLemmatizer #Same word form induction, used for dimensionality reduction\n","from sklearn.model_selection import train_test_split #split the dataset into training set and test set\n","from sklearn.model_selection import cross_val_score, GridSearchCV #Greedy algorithm automatically adjusts parameters\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer #Text preprocessing based on word frequency\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","import string #string methods\n","import random \n","from keras import Sequential\n","from keras.layers import Embedding, LSTM, Dense, Dropout"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1119,"status":"ok","timestamp":1614463971516,"user":{"displayName":"Song Hongyu","photoUrl":"","userId":"04662424222752472550"},"user_tz":-480},"id":"Q7uRZ3OoBCf5","outputId":"c5489959-c1fb-4063-c217-5d3ebf55502f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['reviewerid', 'asin', 'reviewername', 'helpful', 'reviewtext',\n","       'overall', 'summary', 'unixreviewtime', 'reviewtime'],\n","      dtype='object')\n","reviewerid        0\n","asin              0\n","reviewername      1\n","helpful           0\n","reviewtext        0\n","overall           0\n","summary           0\n","unixreviewtime    0\n","reviewtime        0\n","dtype: int64\n","Now the shape of our data is 300 x 9.\n"]}],"source":["data_raw = pd.read_csv('/content/drive/MyDrive/Final_project/dataset_amazon/fuzzydataset.csv')\n","data_raw.head()\n","data_raw.columns = data_raw.columns.str.lower() #Change the name of each column to lowercase \n","print(data_raw.columns) #Preview each column name\n","print(data_raw.isnull().sum()) #Confirm that there is no blank data in reviewtext and summary.\n","print('Now the shape of our data is {} x {}.'.format(data_raw.shape[0], data_raw.shape[1])) #Preview the dimension of data"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1254,"status":"ok","timestamp":1614463974842,"user":{"displayName":"Song Hongyu","photoUrl":"","userId":"04662424222752472550"},"user_tz":-480},"id":"o6sCb40uCtM2"},"outputs":[],"source":["#use sentiment classes to describe the overall values\n","data_raw['sentiment'] = data_raw.overall.replace({\n","    1:'negative',\n","    2:'negative',\n","    3:'neutral',\n","    4:'positive',\n","    5:'positive'\n","})"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1286,"status":"ok","timestamp":1614463978538,"user":{"displayName":"Song Hongyu","photoUrl":"","userId":"04662424222752472550"},"user_tz":-480},"id":"Je375FlCCusk"},"outputs":[],"source":["#create the nature language data with summary + review, and the sentiment is the output\n","X_raw = data_raw['reviewtext'] + ' ' + data_raw['summary']\n","y_test = data_raw['sentiment']\n","#convert the  raw language data into string\n","X_raw = X_raw.astype(str)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3241,"status":"ok","timestamp":1614463983463,"user":{"displayName":"Song Hongyu","photoUrl":"","userId":"04662424222752472550"},"user_tz":-480},"id":"ZSdN5wApC6E0"},"outputs":[],"source":["#Preprocessing of language data\n","def nlpre(X_raw):\n","    \n","    ##sub functions\n","    #punctuations remove function\n","    def remove_punct(X_fcn):\n","        string1 = X_fcn.lower()\n","        translation_table = dict.fromkeys(map(ord, string.punctuation),' ') \n","        string2 = string1.translate(translation_table)\n","        return string2\n","    #stopwords remove function  \n","    def remove_stopwords(X_fcn):\n","        pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*') \n","        string2 = pattern.sub(' ', X_fcn)\n","        return string2\n","    #tokenizing function \n","    def tokenize_words(X_fcn):\n","        words = nltk.word_tokenize(X_fcn) \n","        return words\n","    #lemmatizing function \n","    lemmatizer = WordNetLemmatizer()\n","    def lemmatize_words(X_fcn):\n","        words = lemmatizer.lemmatize(X_fcn) \n","        return words\n","\n","    ##Processing part\n","    X_raw_clear_punct = []\n","    for i in range(len(X_raw)):\n","        test_data = remove_punct(X_raw[i]) #replace !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ with space\n","        X_raw_clear_punct.append(test_data)\n","        \n","    X_raw_clear_stopwords = []\n","    for i in range(len(X_raw)):\n","        test_data = remove_stopwords(X_raw[i]) #remove the words that does not have so much meaning in this process\n","        X_raw_clear_stopwords.append(test_data)\n","        \n","    X_raw_tokenized_words = []\n","    for i in range(len(X_raw)):\n","        test_data = tokenize_words(X_raw[i]) #devide the string into substring\n","        X_raw_tokenized_words.append(test_data)\n","   \n","    X_raw_lemmatized_words = []\n","    for i in range(len(X_raw)):\n","        test_data = lemmatize_words(X_raw[i]) #merge the same words\n","        X_raw_lemmatized_words.append(test_data)\n","        \n","    #creating the bag of words model\n","    cv = CountVectorizer(max_features=1000) #Declare a word to word vector transformationï¼Œspecifies the number of elements of the vectorized corpus\n","    X_raw_vector = cv.fit_transform(X_raw_lemmatized_words).toarray() #convert to matrix\n","    \n","    tfidf = TfidfTransformer()\n","    X_processed_tfidf = tfidf.fit_transform(X_raw_vector).toarray() #scale down the impact of tokens that occur very frequently in a given corpus\n","    \n","    return X_processed_tfidf\n","    #use nlp preprocessing and split dataset into training set and test set\n","X_test = nlpre(X_raw)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1338,"status":"ok","timestamp":1614463985954,"user":{"displayName":"Song Hongyu","photoUrl":"","userId":"04662424222752472550"},"user_tz":-480},"id":"YbFwhmmHDDCZ","outputId":"f2ca568e-1479-4f64-dfcc-4f5ca3048a6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of Label tensor:  (300, 3)\n"]}],"source":["# convert sentiments state in y_test to dummies\n","y_test_dummies = pd.get_dummies(y_test).values\n","print('Shape of Label tensor: ', y_test_dummies.shape)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3640,"status":"ok","timestamp":1614464033424,"user":{"displayName":"Song Hongyu","photoUrl":"","userId":"04662424222752472550"},"user_tz":-480},"id":"Egz-eMSRDKA5","outputId":"1fb79e09-deda-41ba-9e2b-80c6d3733c20"},"outputs":[{"name":"stdout","output_type":"stream","text":["10/10 [==============================] - 2s 204ms/step - loss: 0.3057 - accuracy: 0.9300\n","test accuracy:  93.00000071525574 %\n"]}],"source":["# model test\n","from keras.models import load_model\n","\n","model = load_model('/content/drive/MyDrive/Final_project/model/Amazon_Instru_Sentiment_Analy.h5')\n","scores = model.evaluate(X_test, y_test_dummies)\n","\n","LSTM_accuracy = scores[1]*100\n","\n","print('test accuracy: ', LSTM_accuracy, '%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OzLg1nXZEViv"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM1+NqnaGHADUCDULx6BjXp","name":"final_project_test_Hongyu_Song.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"version":"3.8.5-final"}},"nbformat":4,"nbformat_minor":0}